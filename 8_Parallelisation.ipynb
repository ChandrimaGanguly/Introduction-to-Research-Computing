{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation\n",
    "\n",
    "Sometimes projects you work on will be too large for even a highly optimised code to run on your desktop/laptop.  In this case the only way to get performance benefits is to write the code to take advantage of multiple CPUs. To do this can be tricky and is something you should consider in the prototyping stage as often serial code can prove impossible to paralleise efficiently without wholesale changes.\n",
    "\n",
    "Parallelisation is becoming more important as in since 2005 serial performance of CPU's has not improved but the parallel performance has increased by 32x per core. Also machines are getting more CPU's as standard.\n",
    "\n",
    "The parallelisation model you settle on for your code should also take into account of the likely computer architecture you want to run on and you will need to consider tradeoffs between memory constrants and communication constraints.  In this section we will have a look at some of the issues we face when we try to think parallel\n",
    "\n",
    "## Parallelisation levels\n",
    "\n",
    "There are several levels in which you can paralleise your code\n",
    "\n",
    "0. Multiple serial jobs. The first option for any code as it has perfect scaling and almost no overhead\n",
    "1. Data level parallelisim like SIMD (single instruction multiple data) or Vectorisation. Works on single CPU\n",
    "2. Multicore or Thread based parallelism. Works on multiple CPU's on the same machine\n",
    "3. Distributed Computing or Process based paralellism.  Uses multiple machines/nodes\n",
    "\n",
    "The 0th order approach is to just run multiple instances of your serial code. This works best when scanning parameter space or processing multiple data files.  You just break your inputs into blocks are run many simultaneous jobs each processing their own block.  If you can do this, do it.  It is simple, scales perfectly and is trivial to implement.  You will never write parallel code that out-performs this solution.\n",
    "\n",
    "The 1st, vectorisation, is hard to control in python. You mainly access it by using optimised libraries which are written in a compiled code like C where there are lots of things you can do, this is why you should try to use numpy and scipy for calculations.  An example of vectorisation is when you apply an operation to a array.  The array can then be read into memory in blocks, which match the size of the CPU register, and operated on simultaneously with the same instruction.  If the register can hold 8 floating point numbers then you get a speed up of a factor 8.\n",
    "\n",
    "The 2nd, thread based parallelism, can be done in python with the `threading` or `multiprocessing` packages.  This creates multiple threads which share the same data and can all operate on it.  Unfortunatly this is usually pointless due to the Global Interpreter Lock (GIL).  The GIL blocks more than one thread from accessing the interpreter at a time so only one thread can do anything at one time.  As a result threaded code in python seldom runs any faster (there are some exceptions).  This is a general problem with python in that as it is designed to be interactive it is inherently opposed to parallelism.  Numpy does (sometimes) release the GIL so you can get a benefit with multi-threads, otherwise see https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html.  We will revisit this when we come to Cython which supports `OpenMP` which is the industry standard for thread parallelisim and here we can do it much more easily.  You can however utilise packages which support threading as they are compiled in C so avoid the GIL.\n",
    "\n",
    "The third, process based parallelisim, can be done in python as it creates multiple interpreters so the GIL is not a problem.  In this case we make multiple seperate instances of your code which run independently and can communicate with each other.  This is the case we will look at here with the package `mpi4py` which wraps the industry standard Message Passing Interface (MPI) used in high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI in python\n",
    "\n",
    "First we should note that parallel computing is hard.  You need to think carefully about what you are doing as parallel programmes can be difficult to debug as most debuggers can't track multiple prcessors.  Let's begin with a classic analogy to get us used to the idea of parallel computing, painting a heptagonal wall with a lovely rainbow pattern.\n",
    "\n",
    "You have:\n",
    "1. 7 painters who are ready to paint\n",
    "2. 7 pots of paint, one in each colour of the rainbow\n",
    "3. 7 walls that need to be painted with 7 stripes\n",
    "\n",
    "What is the best way to co-ordinate this?  For the analogy to work we have to assume the painters are fixed as they are the CPU's and in high performance computing \"take the back off the computer and move the chips around\" is seldom the best approach.  It is fairly obvious the we should get each painter to paint a stripe in one colour then pass the paint to the right.  When they recieve the paint from the left they can then paint the next stripe.  If the do this 6 times the wall is finished.\n",
    "\n",
    "Now we can translate this to MPI speak.\n",
    "1. Each painter is a `rank` which controls one process\n",
    "2. Each pot of paint represents a data process\n",
    "3. Each wall represents a block of data.\n",
    "4. Passing the paint is a `point to point message` either a `send` or a `recieve`\n",
    "\n",
    "The main point is that it would be ridiculoues to keep the paint and to try to pass the walls instead.  Similarly the goal of parallelisation is to complete the process with the smallest amount of communication possible.  This is because communication is usually expensive, somewhere between reading from memory and reading from disk (the one it's closer to is very system dependent). As such 'distributed' usually applies to the largest data objects.\n",
    "\n",
    "Simple models of using MPI in code would look like:\n",
    "\n",
    "1. Take a list of tasks then break it into sections which are distributed to each rank (process).  Each process completes its tasks independently then sends the results back to rank 0 which collates the answers.\n",
    "\n",
    "2. Evolve a large simulation which is distributed across multiple ranks.  Before each time step each rank sends the state of it's cells to its neighbours.  Now the time step can be carried out on each section and the new state can be communicated and we keep repeating this sequence.\n",
    "\n",
    "3. Rank 0 has a long list of seperate tasks that need completion.  It distributres an initial task each of the other ranks. Then when they return the result to rank 0 it passes them the next task in the list. This is a master-slave system which is useful for large numbers of ranks as no processes wait for messages.  It's unsuitable for small numbers as obviously rank 0 does nothing other than coordinate so is wasted.\n",
    "\n",
    "### Basic commands\n",
    "\n",
    "Let's try some simple examples to see what MPI commands look like:\n",
    "\n",
    "**Example 1:**\n",
    "Here we just sent a message to each of the other ranks to say hello using `send` and `recv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Contents of Code/mpitest1.py (example x will be in file mpitest'x'.py)\n",
    "# Run this with: mpiexec -n 8 python mpitest1.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Set up communication stuff\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# Find out which rank I am\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Find out how many ranks there are\n",
    "size = comm.Get_size()\n",
    "\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "# Send a message to all other ranks\n",
    "for i in range(1,size):\n",
    "#   Decide who I will send the message to\n",
    "#   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "#   Send the message, tag is not required here but is usefull later\n",
    "    comm.send(\"Hello from {}\".format(rank), dest=rank_send, tag=11)\n",
    "#   collect my message\n",
    "    message = comm.recv(source=rank_recv, tag=11)\n",
    "#   Print what happened\n",
    "    print(\"[{}] sent:{} recv:{} Message: {}\".format(rank,rank_send,rank_recv,message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag helps you keep track when you recieve multiple messages eg: I could send 5 messages from 0 to 1 and tag would tell me which was which.\n",
    "\n",
    "**Example 2:**\n",
    "Send an array or buffer type object to the other ranks.  Note here we must use `Send` and `Recv` not `send` and `recv`.  This capitilisation difference exists for all commands.  From here on we will only demonstrate one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 6, 7, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "    comm.Send(data_s, dest=rank_send, tag=11)\n",
    "    comm.Recv(data_r, source=rank_recv, tag=11)\n",
    "    \n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**\n",
    "This type of operation where you are passing information around the ranks is best done with `sendrecv` which combines the two (we will discuss why later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "    comm.Sendrecv(data_s, dest=rank_send, recvbuf=data_r, source=rank_recv)\n",
    "    \n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**\n",
    "Send data from one rank to all others using a broadcast `bcast` (`Bcast` works the same here except `None` becomes an empty array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank==0:\n",
    "    data = \"0 is the best rank\"\n",
    "else:\n",
    "    data = None\n",
    "    \n",
    "data = comm.bcast(data, root=0)\n",
    "\n",
    "print(\"[{}] I just heard that {}\".format(rank,data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5:**\n",
    "More usefully we can break a data object up and send a part to each rank with `Scatter` or put it back together with `Gather`.  There is also `Allgather` for a `Gather` +`Bcast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    data_s = np.arange(5*size)\n",
    "else:\n",
    "    data_s = None\n",
    "\n",
    "comm.Scatter(data_s,data_r,root=0)\n",
    "\n",
    "print(\"[{}] My data is \".format(rank),data_r)\n",
    "\n",
    "if rank==3:\n",
    "    data_s = np.empty(5*size, dtype='int')\n",
    "\n",
    "comm.Gather(data_r,data_s,root=3)\n",
    "\n",
    "print(\"[{}] My data now is \".format(rank),data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also `alltoall` which combines both\n",
    "\n",
    "**Example 6:**\n",
    "The most useful (in my opinion anyway) is the global reduce which takes buffers form each rank then combines them with an operation like: `MIN`, `MAX`, `SUM`, `PROD`,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(5))\n",
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "print( \"[{}] Starting with data: \".format(rank),data_s )\n",
    "\n",
    "comm.Reduce(data_s,data_r,op=MPI.SUM,root=0)\n",
    "\n",
    "if rank==0:\n",
    "    print(\"summed data: \",data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have `Allreduce` which is a `Reduce` combined with a `Bcast`.  One other usefull command is `Barrier` which has no arguments and simply holds all processes until all reach it so forces syncronisation.  Don't be tempted to use this unless you have to as it juts slows down your code.\n",
    "\n",
    "All the above commands are blocking commands in that the process doesn't move on until they are complete.  This means that we always have to wait until comunications have completed before we can continue with our code.  Communication is usually slow so these are also non-blocking versions of `send` and `recv` we can use.  This allows us to send information then continue calculation while we wait for it to arrive.  The non-blocking versions are called `Isend` and `Irecv` and work like this:\n",
    "\n",
    "**Example 7:**\n",
    "Use non-blocking commands so we can do stuff while we wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(200))\n",
    "data_r = np.zeros((200), dtype='int')\n",
    "\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "#   Set both send and recieve, order doesn't matter as they don't block\n",
    "    request = comm.Irecv(data_r, source=rank_recv, tag=11)\n",
    "    comm.Isend(data_s, dest=rank_send, tag=11)\n",
    "    \n",
    "#   Count while I wait for message to arrive\n",
    "    i=0\n",
    "    while not request.Get_status():\n",
    "        i+=1\n",
    "        \n",
    "print(\"[{}] while waiting I counted to \".format(rank),i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlocks\n",
    "\n",
    "One key thing to watch our for when using MPI is deadlocks.  This is when you have one rank stuck waiting for a message that onother rank can't send.  This usually happens when you get you send\\recv out of order which is easier to do that you think as you never know the order the ranks will meet the code.  It is possible you already met this as some of the examples are not safe.  The first and second example can both deadlock if they happen to be executed at almost exactly the same time.  \n",
    "\n",
    "What happens if all ranks enter the `send` at the same time? As the sends are blocking sends they won't complete until the data is recieved by `recv`. But as all ranks are held in the `send` command, none make it to the `recv` and the code stalls forever.  Here this is unlikely as the message is short and the communication is fast and all we need is for the first rank to exit `send` before the last starts to avoid a deadlock.  This is the danger with parallel programming.  Small tests like this can be fine but if I then ported the code to a large machine with slow communication and I was sending large amounts of data this may happen most of the time.  This is why we have `sendrecv` which both sends and waits for data so can't deadlock. This solution forces the code to syncronise which can slow the code down.  The alternative is to use non-blocking communication which is more efficent but you have to check the messages complete before you try to use the data sent.\n",
    "\n",
    "The best idea it to try to use collectives wherever possible then non-blocking then blocking\n",
    "\n",
    "You should note that collectives can be either blocking or non-blocking depending on the implementation so don't rely on them to sync your ranks.  Also make sure all ranks see the same collectives in the same order as this may deadlock the code too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Breaking a loop: Take the code in triangle_int.py and parallelise it to split the caclulation of $E_{ijk}$ across 4 processes.\n",
    "\n",
    "\n",
    "2. Decomposing a domain: Parallelise the code in mpi_GOL.py to split the game of life across 4 ranks for a 10x40 game board.  Initial starting position is defined inside.  After 60 iterations the picture of a \"duck\" should be on the last tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on mpi4py can be found here:\n",
    "\n",
    "https://info.gwdg.de/~ceulig/docs-dev/doku.php?id=en:services:application_services:high_performance_computing:mpi4py\n",
    "\n",
    "https://mpi4py.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
