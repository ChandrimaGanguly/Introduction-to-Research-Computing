{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation\n",
    "\n",
    "Sometimes projects you work on will be too large for even a highly optimised code to run on your desktop/laptop.  In this case the only way to get performance benefits is to write the code to take advantage of multiple CPUs. To do this can be tricky and is something you should consider in the prototyping stage as often serial code can prove impossible to paralleise efficiently without wholesale changes.\n",
    "\n",
    "Parallelisation is becoming more important as in since 2005 serial performance of CPU's has not improved but the parallel performance has increased by 32x per core. Also machines are getting more CPU's as standard.\n",
    "\n",
    "![](Plots/CPUClock.png)\n",
    "\n",
    "The parallelisation model you settle on for your code should also take into account of the likely computer architecture you want to run on and you will need to consider tradeoffs between memory constrants and communication constraints.  In this section we will have a look at some of the issues we face when we try to think parallel\n",
    "\n",
    "## Parallelisation levels\n",
    "\n",
    "There are several levels in which you can paralleise your code\n",
    "\n",
    "0. Multiple serial jobs. The first option for any code as it has perfect scaling and almost no overhead\n",
    "1. CPU level parallelisim like SIMD (single instruction multiple data) or Vectorisation. Works on single CPU, mostly done by compilers\n",
    "2. Multicore or Thread based parallelism. Works on multiple CPU's on the same machine\n",
    "3. Distributed Computing or Process based paralellism.  Uses multiple machines/nodes\n",
    "\n",
    "The 0th order approach is to just run multiple instances of your serial code. This works best when scanning parameter space or processing multiple data files.  You just break your inputs into blocks are run many simultaneous jobs each processing their own block.  If you can do this, do it.  It is simple, scales perfectly and is trivial to implement.  You will never write parallel code that out-performs this solution.\n",
    "\n",
    "The 1st, vectorisation, is hard to control in python. You mainly access it by using optimised libraries which are written in a compiled code like C where there are lots of things you can do, this is why you should try to use numpy and scipy for calculations.  An example of vectorisation is when you apply an operation to a array.  The array can then be read into memory in blocks, which match the size of the CPU register, and operated on simultaneously with the same instruction.  If the register can hold 8 floating point numbers then you get a speed up of a factor 8 (in theory).\n",
    "\n",
    "The 2nd, thread based parallelism, can be done in python with the `threading` or `multiprocessing` packages.  This creates multiple threads which share the same data and can all operate on it.  Unfortunatly this is usually pointless due to the Global Interpreter Lock (GIL).  The GIL blocks more than one thread from accessing the interpreter at a time so only one thread can do anything at one time.  As a result threaded code in python seldom runs any faster (there are some exceptions).  This is a general problem with python in that as it is designed to be interactive it is inherently opposed to parallelism.  Numpy does (sometimes) release the GIL so you can get a benefit with multi-threads, otherwise see https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html.  We will revisit this when we come to Cython which supports `OpenMP` which is the industry standard for thread parallelisim and here we can do it much more easily.  You can however utilise packages which support threading as they are compiled in C so avoid the GIL.\n",
    "\n",
    "The third, process based parallelisim, can be done in python as it creates multiple interpreters so the GIL is not a problem.  In this case we make multiple seperate instances of your code which run independently and can communicate with each other.  This is the case we will look at here with the package `mpi4py` which wraps the industry standard Message Passing Interface (MPI) used in high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI in python\n",
    "\n",
    "First we should note that parallel computing is hard.  You need to think carefully about what you are doing as parallel programmes can be difficult to debug as most debuggers can't track multiple processors.  Let's begin with a classic analogy to get us used to the idea of parallel computing, painting a heptagonal wall with a lovely rainbow pattern.\n",
    "\n",
    "You have:\n",
    "1. 7 painters who are ready to paint\n",
    "2. 7 pots of paint, one in each colour of the rainbow\n",
    "3. 7 walls that need to be painted with 7 stripes\n",
    "\n",
    "What is the best way to co-ordinate this?  For the analogy to work we have to assume the painters are fixed as they are the CPU's and in high performance computing \"take the back off the computer and move the chips around\" is seldom the best approach.  It is fairly obvious the we should get each painter to paint a stripe in one colour then pass the paint to the right.  When they recieve the paint from the left they can then paint the next stripe.  If the do this 6 times the wall is finished.\n",
    "\n",
    "Now we can translate this to MPI speak.\n",
    "1. Each painter is a `rank` which controls one process\n",
    "2. Each pot of paint represents a data process\n",
    "3. Each wall represents a block of data.\n",
    "4. Passing the paint is a `point to point message` either a `send` or a `recieve`\n",
    "\n",
    "The main point is that it would be ridiculoues to keep the paint and to try to pass the walls instead.  Similarly the goal of parallelisation is to complete the process with the smallest amount of communication possible.  This is because communication is usually expensive, somewhere between reading from memory and reading from disk (the one it's closer to is very system dependent). As such 'distributed' usually applies to the largest data objects.\n",
    "\n",
    "Simple models of using MPI in code would look like:\n",
    "\n",
    "1. Take a list of tasks then break it into sections which are distributed to each rank (process).  Each process completes its tasks independently then sends the results back to rank 0 which collates the answers.\n",
    "\n",
    "2. Evolve a large simulation which is distributed across multiple ranks.  Before each time step each rank sends the state of it's cells to its neighbours.  Now the time step can be carried out on each section and the new state can be communicated and we keep repeating this sequence.\n",
    "\n",
    "3. Rank 0 has a long list of seperate tasks that need completion.  It distributres an initial task each of the other ranks. Then when they return the result to rank 0 it passes them the next task in the list. This is a master-slave system which is useful for large numbers of ranks as no processes wait for messages.  It's unsuitable for small numbers as obviously rank 0 does nothing other than coordinate so is wasted.\n",
    "\n",
    "### Basic commands\n",
    "\n",
    "Let's try some simple examples to see what MPI commands look like:\n",
    "\n",
    "**Example 1:**\n",
    "Here we just sent a message to each of the other ranks to say hello using `send` and `recv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Contents of Code/mpitest1.py (example x will be in file mpitest'x'.py)\n",
    "# Run this with: mpiexec -n 8 python mpitest1.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Set up communication stuff\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# Find out which rank I am\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Find out how many ranks there are\n",
    "size = comm.Get_size()\n",
    "\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "# Send a message to all other ranks\n",
    "for i in range(1,size):\n",
    "#   Decide who I will send the message to\n",
    "#   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "#   Send the message, tag is not required here but is usefull later\n",
    "    comm.send(\"Hello from {}\".format(rank), dest=rank_send, tag=11)\n",
    "#   collect my message\n",
    "    message = comm.recv(source=rank_recv, tag=11)\n",
    "#   Print what happened\n",
    "    print(\"[{}] sent:{} recv:{} Message: {}\".format(rank,rank_send,rank_recv,message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag helps you keep track when you recieve multiple messages eg: I could send 5 messages from 0 to 1 and tag would tell me which was which.\n",
    "\n",
    "**Example 2:**\n",
    "Send an array or buffer type object to the other ranks.  Note here we must use `Send` and `Recv` not `send` and `recv`.  This capitilisation difference exists for all commands.  From here on we will only demonstrate one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 6, 7, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "    comm.Send(data_s, dest=rank_send, tag=11)\n",
    "    comm.Recv(data_r, source=rank_recv, tag=11)\n",
    "    \n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**\n",
    "This type of operation where you are passing information around the ranks is best done with `sendrecv` which combines the two (we will discuss why later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "    comm.Sendrecv(data_s, dest=rank_send, recvbuf=data_r, source=rank_recv)\n",
    "    \n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**\n",
    "Send data from one rank to all others using a broadcast `bcast` (`Bcast` works the same here except `None` becomes an empty array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank==0:\n",
    "    data = \"0 is the best rank\"\n",
    "else:\n",
    "    data = None\n",
    "    \n",
    "data = comm.bcast(data, root=0)\n",
    "\n",
    "print(\"[{}] I just heard that {}\".format(rank,data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5:**\n",
    "More usefully we can break a data object up and send a part to each rank with `Scatter` or put it back together with `Gather`.  There is also `Allgather` for a `Gather` +`Bcast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    data_s = np.arange(5*size)\n",
    "else:\n",
    "    data_s = None\n",
    "\n",
    "comm.Scatter(data_s,data_r,root=0)\n",
    "\n",
    "print(\"[{}] My data is \".format(rank),data_r)\n",
    "\n",
    "if rank==3:\n",
    "    data_s = np.empty(5*size, dtype='int')\n",
    "\n",
    "comm.Gather(data_r,data_s,root=3)\n",
    "\n",
    "print(\"[{}] My data now is \".format(rank),data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also `alltoall` which combines both\n",
    "\n",
    "**Example 6:**\n",
    "The most useful (in my opinion anyway) is the global reduce which takes buffers form each rank then combines them with an operation like: `MIN`, `MAX`, `SUM`, `PROD`,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(5))\n",
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "print( \"[{}] Starting with data: \".format(rank),data_s )\n",
    "\n",
    "comm.Reduce(data_s,data_r,op=MPI.SUM,root=0)\n",
    "\n",
    "if rank==0:\n",
    "    print(\"summed data: \",data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have `Allreduce` which is a `Reduce` combined with a `Bcast`.  One other usefull command is `Barrier` which has no arguments and simply holds all processes until all reach it so forces syncronisation.  Don't be tempted to use this unless you have to as it just slows down your code.\n",
    "\n",
    "All the above commands are blocking commands in that the process doesn't move on until they are complete.  This means that we always have to wait until comunications have completed before we can continue with our code.  Communication is usually slow so these are also non-blocking versions of `send` and `recv` we can use.  This allows us to send information then continue calculation while we wait for it to arrive.  The non-blocking versions are called `Isend` and `Irecv` and work like this:\n",
    "\n",
    "**Example 7:**\n",
    "Use non-blocking commands so we can do stuff while we wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = np.random.randint(1,9,(200))\n",
    "data_r = np.zeros((200), dtype='int')\n",
    "\n",
    "\n",
    "for i in range(1,size):\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    \n",
    "#   Set both send and recieve, order doesn't matter as they don't block\n",
    "    request = comm.Irecv(data_r, source=rank_recv, tag=11)\n",
    "    comm.Isend(data_s, dest=rank_send, tag=11)\n",
    "    \n",
    "#   Count while I wait for message to arrive\n",
    "    i=0\n",
    "    while not request.Get_status():\n",
    "        i+=1\n",
    "        \n",
    "print(\"[{}] while waiting I counted to \".format(rank),i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlocks\n",
    "\n",
    "One key thing to watch our for when using MPI is deadlocks.  This is when you have one rank stuck waiting for a message that another rank can't send.  This usually happens when you get you send\\recv out of order which is easier to do that you think as you never know the order the ranks will meet the code.  It is possible you already met this as some of the examples are not safe.  The first and second example can both deadlock if they happen to be executed at almost exactly the same time.  \n",
    "\n",
    "What happens if all ranks enter the `send` at the same time? As the sends are blocking sends they won't complete until the data is recieved by `recv`. But as all ranks are held in the `send` command, none make it to the `recv` and the code stalls forever.  Here this is unlikely as the message is short and the communication is fast and all we need is for the first rank to exit `send` before the last starts to avoid a deadlock.  This is the danger with parallel programming.  Small tests like this can be fine but if I then ported the code to a large machine with slow communication and I was sending large amounts of data this may happen most of the time.  This is why we have `sendrecv` which both sends and waits for data so can't deadlock. This solution forces the code to syncronise which can slow the code down.  The alternative is to use non-blocking communication which is more efficent but you have to check the messages complete before you try to use the data sent.  Collectives can also block is not all ranks can get to them.  For example this will stall forever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    data_s = np.arange(5*size)\n",
    "else:\n",
    "    data_s = None\n",
    "\n",
    "if rank==0:\n",
    "    comm.Scatter(data_s,data_r,root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best idea it to try to use collectives wherever possible then non-blocking then blocking\n",
    "\n",
    "You should note that collectives can be either blocking or non-blocking depending on the implementation so don't rely on them to sync your ranks.  Also make sure all ranks see the same collectives in the same order as this may deadlock the code too (basicly don't use them in if statements like above)\n",
    "\n",
    "## Parallel patterns\n",
    "\n",
    "There are lots of different approaches to parallelising code.  The best one for your code is quite problem specific.  Here we will look as some basic \"patterns\" used to parallelise code.  First there are a few things to consider.\n",
    "\n",
    "- How much of my code can be parallelised? If 30% of your code is serial then the maximum speedup you can achieve is $1/0.3 = 3.3$x.  This gives you a benchmark to work against and whether the effort will be worth it.\n",
    "\n",
    "- How will my code scale?  If you plan to use you code for large problems but only use a small test data set you need to check how you approach will work on the full solution. Suppose you have two approaches the first is faster on the small problem but scales as $n^2$ (eg gaussian smoothing in 2D) the second is slower but scales as $n\\ln(n)$ (eg gaussian smoothing via fft then weighting modes then ifft back).  Here the second would be best for the large problem\n",
    "\n",
    "- Load balance.  How will you ensure that each CPU has the same amount of work to do?\n",
    "\n",
    "Parallelisation also has costs\n",
    "\n",
    "- Efficency (Sometimes we will chose algorithms that paralleise well but are slower for serial)\n",
    "- Simplicity (Code get harder to read and maintain)\n",
    "- Portability (parallel code is often writn with a particulare system in mind so it can be hard to migrate to new machines)\n",
    "\n",
    "\n",
    "With this lets look at some simple paralleisation patterns (in order of complexity)\n",
    "\n",
    "1. Task parallelisim\n",
    "2. Domain decomposition\n",
    "3. Divide and conquer\n",
    "4. Recursive data\n",
    "5. Pipelines\n",
    "\n",
    "### Task parallelisim\n",
    "\n",
    "Here we have a list of independent tasks that need to be completed and we just divide them up over the ranks.  The best example is splitting a loop like in the following example:\n",
    "\n",
    "**Pattern 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of Code/mpipattern1.py (pattern x will be in file mpipattern'x'.py)\n",
    "# Run this with: mpiexec -n 8 python mpipattern1.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "def divide_loops(loops,size):\n",
    "    loop_rank=loops//size\n",
    "    auxloop = loops%size\n",
    "    start_loop = rank*loop_rank\n",
    "    end_loop = (rank+1)*loop_rank\n",
    "    \n",
    "    if(auxloop!=0):\n",
    "        if (rank < auxloop):\n",
    "            start_loop = start_loop + rank\n",
    "            end_loop = end_loop + rank + 1\n",
    "        else:\n",
    "            start_loop = start_loop + auxloop\n",
    "            end_loop = end_loop + auxloop\n",
    "\n",
    "    return start_loop, end_loop\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# sum numbers from 1 to 1000\n",
    "\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "loops = 1000\n",
    "\n",
    "s,e = divide_loops(loops,size)\n",
    "\n",
    "x=0\n",
    "for i in range(s,e):\n",
    "    x += i+1\n",
    "\n",
    "print( \"[{}] Local sum {}\".format(rank,x) )\n",
    "\n",
    "y = comm.reduce(x,op=MPI.SUM,root=0)\n",
    "if rank==0:\n",
    "    print( \"[{}] Total sum {}\".format(rank,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple loops it is usually best to flatten them.  An example is looping over $i,j$ where $i<=j$.  Here splitting on $i$ would be bad as the ranks would get different amounts of work to do affecting load balance. Instead flatten them into a single loop. So to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    }
   ],
   "source": [
    "imax = 10\n",
    "x=0\n",
    "for ii in range(imax):\n",
    "    for jj in range(ii,imax):\n",
    "        x += ii+jj\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do:\n",
    "\n",
    "**Pattern 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "def divide_loops(loops,size):\n",
    "    loop_rank=loops//size\n",
    "    auxloop = loops%size\n",
    "    start_loop = rank*loop_rank\n",
    "    end_loop = (rank+1)*loop_rank\n",
    "    \n",
    "    if(auxloop!=0):\n",
    "        if (rank < auxloop):\n",
    "            start_loop = start_loop + rank\n",
    "            end_loop = end_loop + rank + 1\n",
    "        else:\n",
    "            start_loop = start_loop + auxloop\n",
    "            end_loop = end_loop + auxloop\n",
    "\n",
    "    return start_loop, end_loop\n",
    "\n",
    "def gen(s,e,imax):\n",
    "    count = 1\n",
    "    for ii in range(imax):\n",
    "        for jj in range(ii,imax):\n",
    "            if count>e:\n",
    "                break\n",
    "            if count>s:\n",
    "                yield (ii,jj)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# double sum numbers from 1 to 50\n",
    "\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "\n",
    "imax = 10\n",
    "loops = imax*(imax+1)//2\n",
    "\n",
    "s,e = divide_loops(loops,size)\n",
    "G1 = gen(s,e,imax)\n",
    "\n",
    "x=0\n",
    "for item in G1:\n",
    "    x += item[0]+item[1]\n",
    "\n",
    "print( \"[{}] Local sum {}\".format(rank,x) )\n",
    "\n",
    "y = comm.reduce(x,op=MPI.SUM,root=0)\n",
    "if rank==0:\n",
    "    print( \"[{}] Total sum {}\".format(rank,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large jobs with extremly uneven workloads we could instead opt of a master-slave set up where the jobs are distributed by a master process.   This can be quite tricky to programme but here is an simple (relative to what it could look like) example that calculates $n(n+1)/2$ for $n$ from 0 to 10:\n",
    "\n",
    "**Pattern 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "# 0 is master\n",
    "# rest are slaves\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank==0:\n",
    "    tasklist = (i for i in range(10))\n",
    "    \n",
    "    finished = 0\n",
    "    \n",
    "    status = MPI.Status()\n",
    "    \n",
    "    for i in range(1,size):\n",
    "        try:\n",
    "            message = next(tasklist)\n",
    "        except StopIteration:\n",
    "            message = -1\n",
    "          \n",
    "        print(\"[{}] Sending task: {} to rank {}\".format(rank,message,i))\n",
    "        comm.isend(message, dest=i, tag=i)\n",
    "\n",
    "    while finished<size-1:\n",
    "        flag = comm.iprobe(status=status, source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG)\n",
    "        if flag==True:\n",
    "            source = status.source\n",
    "            test = comm.irecv(source=source, tag=source)\n",
    "            reply = test.wait()\n",
    "            print(\"[{}] Recieving result: {} from rank {}\".format(rank,reply,source))\n",
    "            if reply==-1:\n",
    "                finished +=1\n",
    "                print(\"[{}] Recieved termination finished count: {}\".format(rank,finished))\n",
    "            else:\n",
    "                print(\"[{}] Done with result {}\".format(rank,reply))\n",
    "                try:\n",
    "                    message = next(tasklist)\n",
    "                except StopIteration:\n",
    "                    message = -1\n",
    "                print(\"[{}] Sending task: {} to rank {}\".format(rank,message,i))\n",
    "                comm.isend(message, dest=source, tag=source)\n",
    "\n",
    "\n",
    "else:\n",
    "    while True:\n",
    "        test = comm.irecv(source=0, tag=rank)\n",
    "        task = test.wait()\n",
    "        print(\"[{}] Recieving task: {} from rank {}\".format(rank,task,0))\n",
    "        \n",
    "        if task==-1:\n",
    "            comm.isend(-1, dest=0, tag=rank)\n",
    "            print(\"[{}] Sending termination to rank {}\".format(rank,0))\n",
    "            break\n",
    "        else:\n",
    "            result = task*(task+1)//2\n",
    "            comm.isend(result, dest=0, tag=rank)\n",
    "            print(\"[{}] Sending result {} to rank {}\".format(rank,result,0))\n",
    "\n",
    "print(\"[{}] I'm done\".format(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Decomposition\n",
    "\n",
    "Task decomposition distributes the tasks.  Here we will instead distribute the data. A simple example is multiplication of a $n\\times m$ matrix $M$ by a $n\\times n$ matrix $N$ where $m>>n$.  We split the matrix M up over processors then apply the dot product then gather. This would look like:\n",
    "\n",
    "**Pattern 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "mm = 20\n",
    "split = mm//size\n",
    "\n",
    "matrix_M_r = np.empty((split,5),dtype='int')\n",
    "\n",
    "matrix_N = np.arange(1,26).reshape((5,5))\n",
    "\n",
    "if rank==0:\n",
    "    matrix_M_s = np.array([range(i,i+5) for i in range(mm)])\n",
    "else:\n",
    "    matrix_M_s = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),matrix_M_s)\n",
    "    \n",
    "comm.Scatter(matrix_M_s,matrix_M_r,root=0)\n",
    "\n",
    "print(\"[{}] My recv data is: \".format(rank),matrix_M_r)\n",
    "\n",
    "matrix_M_r = np.dot(matrix_M_r,matrix_N)\n",
    "\n",
    "comm.Gather(matrix_M_r,matrix_M_s,root=0)\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My gath data is \\n\".format(rank),matrix_M_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More usually ths is used for large simulations and so we need the different partitions to comunicate with each other.  Also we should note that there are many different ways to partition the data.  We can divide up the domain the simulation runs on, either into strips or squares (2D).  For nbody type codes where we are tracking particles this could lead to load imbalances when particles congregate in specific cells.  Here we could have an adaptive mesh or we could partition the particles instead.  Our example for domain decomposition with communication is left as an exercise using our \"game of life\" code.\n",
    "\n",
    "### Divide and Conquer\n",
    "\n",
    "This technique is used when dividing or recomdining tasks/data is non trivial so we instead scatter/gather iteritivley.  So our algorith looks like this:\n",
    "\n",
    "![](Plots/DivideConquer.png)\n",
    "\n",
    "A good example is sorting a very long list.  We can divide up the elements easily and sort then but merging the data back is difficult.  Instead here we merge pairs of lists back together and slowly recombine the full list\n",
    "\n",
    "**Pattern 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "levels = int(math.log2(size))\n",
    "\n",
    "def mergelist(list1,list2):\n",
    "    size1 = len(list1)\n",
    "    size2 = len(list2)\n",
    "    size3 = size1+size2\n",
    "    list3 = np.empty((size3),dtype='int')\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    while i<size1 and j<size2:\n",
    "        if list1[i]<list2[j]:\n",
    "            list3[k] = list1[i]\n",
    "            i+=1\n",
    "        else:\n",
    "            list3[k] = list2[j]\n",
    "            j+=1\n",
    "        k+=1\n",
    "    \n",
    "    while i<size1:\n",
    "        list3[k] = list1[i]\n",
    "        i+=1\n",
    "        k+=1\n",
    "        \n",
    "    while j<size2:\n",
    "        list3[k] = list2[j]\n",
    "        j+=1\n",
    "        k+=1\n",
    "    \n",
    "    return list3\n",
    "            \n",
    "length = 200\n",
    "split = length//size\n",
    "\n",
    "list1 = np.empty((split),dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    listall = np.random.randint(0,100,(200))\n",
    "else:\n",
    "    listall = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),listall)\n",
    "    \n",
    "comm.Scatter(listall,list1,root=0)\n",
    "\n",
    "list1 = np.sort(list1)\n",
    "\n",
    "level = 2\n",
    "# now merge back\n",
    "for i in range(1,levels+1):\n",
    "    if rank%2**i == 2**(i-1):\n",
    "        dest = rank - 2**(i-1)\n",
    "        comm.Send(list1, dest=dest, tag=11)\n",
    "    elif rank%2**i == 0:\n",
    "        list2 = np.empty((split*2**(i-1)),dtype='int')\n",
    "        src = rank + 2**(i-1)\n",
    "        comm.Recv(list2, source=src, tag=11)\n",
    "        list1 = mergelist(list1,list2)\n",
    "        \n",
    "if rank==0:\n",
    "    print(list1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive data\n",
    "\n",
    "Sometimes we will want to perform recursive operations on data which are inherently serial.  In these situations we can expose parallelisim by using recasting the problem to expose concurrency.  This can be difficult and often the algorithm is slower but with enough cores the code will still run faster.  A simple example is calcuating a cumulitive sum for a list.  Here we would normaly start at the begining and compute the sum as we progress along the list.  This is an $O(N)$ operation but is serial so can only use one core.  We can instead recast the problem by calculating recursive partial sums like so:\n",
    "\n",
    "![](Plots/Recursive.png)\n",
    "\n",
    "Now the algorithim is $O(N\\ln(N))$ but each element is calculated separately. Now we have to be carefull as if we have fewer than $O(\\ln(N))$ processors/threads then the code will be lower.  This couls still be useful if the list is very large so can't fit in memory. Here is an example of how this might look (This ends up being a divide and conquer algorithm like the previous example):\n",
    "\n",
    "**Pattern 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "levels = int(math.log2(size))\n",
    "\n",
    "length = 200\n",
    "split = length//size\n",
    "\n",
    "list1 = np.empty((split),dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    listall = np.random.randint(0,100,(length))\n",
    "else:\n",
    "    listall = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),listall)\n",
    "    \n",
    "comm.Scatter(listall,list1,root=0)\n",
    "\n",
    "x=0\n",
    "for index, item in enumerate(list1):\n",
    "     x += item\n",
    "     list1[index] = x\n",
    "     \n",
    "for i in range(1,levels+1):\n",
    "    if rank%2**i == 0:\n",
    "        dest = rank + 2**(i-1)\n",
    "        comm.send(list1[-1], dest=dest, tag=11)\n",
    "    elif rank%2**i == 2**(i-1):\n",
    "        src = rank - 2**(i-1)\n",
    "        x = comm.recv(source=src, tag=11)\n",
    "        list1 = list1+x\n",
    "\n",
    "comm.Gather(list1,listall,root=0)\n",
    "\n",
    "if rank==0:\n",
    "    print(listall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Task parallelisim: Take the code in triangle_int.py and parallelise it to split the calculation of $E_{ijk}$ across 4 processes.\n",
    "\n",
    "\n",
    "2. Decomposing a domain: Parallelise the code in mpi_GOL.py to split the game of life across 4 ranks for a 10x40 game board.  Initial starting position is defined inside.  After 60 iterations the picture of a \"duck\" should be on the last tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for mpi4py isn't great but some information can be found here:\n",
    "\n",
    "https://info.gwdg.de/~ceulig/docs-dev/doku.php?id=en:services:application_services:high_performance_computing:mpi4py\n",
    "\n",
    "https://mpi4py.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
